# 浮点类型误差问题

## 现象
```
    >>> 0.1
    0.1
    >>> 0.1+0.2
    0.30000000000000004
    >>> i=0
    >>> while i<1:
    ...     i=i+0.1
    ...     print(i)
    ...
    0.1
    0.2
    0.30000000000000004
    0.4
    0.5
    0.6
    0.7
    0.7999999999999999
    0.8999999999999999
    0.9999999999999999
    1.0999999999999999
    >>> 0.3==0.1+0.2
    False
    >>>
```
    以上现象发现python中浮点数计算是不精确的的
## 分析原因
    
        Python 中的 float 类型使用C语言的 double 类型进行存储。 float 对象的值是以固定的精度（通常为 53 位）存储的二进制浮点数，
    由于 Python 使用 C 操作，而后者依赖于处理器中的硬件实现来执行浮点运算。 这意味着就浮点运算而言，Python 的行为类似于许多流行的语
    言，包括 C 和 Java。
        以0.1为例，其二进制表示为0.0001100110011001100110011001100110011001100110011001101穷尽于0.1
    
## 解决方法

> * 方案一
        
        使用decimal库
```
        import decimal
        a = decimal.Decimal("10.0")
        b = decimal.Decimal("3")
        print(10.0/3)
        print(a/b)
        print(round(3.6812,2))
        print(round(3.6862,2))
        ------------
        3.3333333333333335
        3.333333333333333333333333333
        3.68
        3.69
```
              
> * 方案二

        用整数代表小数，具体是如果保留小数点后两位，则用0.01使用1来代替
    
    
    

